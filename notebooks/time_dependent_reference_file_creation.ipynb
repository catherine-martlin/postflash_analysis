{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "postflash_data = pd.read_pickle('/grp/hst/wfc3u/postflash_2021/flt_postflash_with_stats.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullframe_pf = postflash_data.loc[(postflash_data['subarray'] == False)] \n",
    "fullframe_pf_A = postflash_data.loc[(postflash_data['subarray'] == False) & (postflash_data['shutter'] == 'A') & (postflash_data['flash_cur'] == 'MED')] \n",
    "fullframe_pf_B = postflash_data.loc[(postflash_data['subarray'] == False) & (postflash_data['shutter'] == 'B') & (postflash_data['flash_cur'] == 'MED')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack(list_of_files,out_file,error_file):\n",
    "    hdr = fits.getheader(list_of_files[0], 1)\n",
    "    nx = hdr['NAXIS1']\n",
    "    ny = hdr['NAXIS2']\n",
    "    nf = len(list_of_files)\n",
    "    data_array_1 = np.empty((nf, ny, nx), dtype=float)\n",
    "    data_array_2 = np.empty((nf, ny, nx), dtype=float)\n",
    "    set_data=fits.getdata(list_of_files[0], 1)\n",
    "    rms_1 = np.zeros(len(list_of_files), dtype=float)\n",
    "    rms_2 = np.zeros(len(list_of_files), dtype=float)\n",
    "    error_array_1 = np.empty((nf, ny, nx), dtype=float)\n",
    "    error_array_2 = np.empty((nf, ny, nx), dtype=float)\n",
    "    total_error_1 = np.zeros_like(set_data, dtype=float)\n",
    "    total_error_2 = np.zeros_like(set_data, dtype=float)\n",
    "    for i , f in enumerate(list_of_files):\n",
    "        data_1=fits.getdata(f, 1)\n",
    "        data_2=fits.getdata(f, 4)\n",
    "        error_1=fits.getdata(f, 2)\n",
    "        error_2=fits.getdata(f, 5)\n",
    "        DQ_1=fits.getdata(f, 3)\n",
    "        DQ_2=fits.getdata(f, 6)\n",
    "        #data_1=data1\n",
    "        #data_2=data2\n",
    "        mask_1=np.zeros_like(data_1, dtype=bool)\n",
    "        mask_2=np.zeros_like(data_2, dtype=bool)\n",
    "        mask_1[DQ_1>=2**13]= True\n",
    "        mask_2[DQ_2>=2**13]= True\n",
    "        error_1[(0<DQ_1 & (DQ_1<2**13))]=0.00001\n",
    "        error_2[(0<DQ_2& (DQ_2<2**13))]=0.00001\n",
    "        error_1_sq=error_1**2\n",
    "        error_2_sq=error_2**2\n",
    "        masked_data_1= ma.array(data=data_1, mask=mask_1)\n",
    "        masked_data_2= ma.array(data=data_2, mask=mask_2)\n",
    "        data_array_1[i, :, :] = masked_data_1\n",
    "        rms_1[i] = masked_data_1.std()\n",
    "        data_array_2[i, :, :] = masked_data_2\n",
    "        rms_2[i] = masked_data_2.std()\n",
    "        total_error_1=total_error_1+(error_1_sq)\n",
    "        total_error_2=total_error_2+(error_2_sq)\n",
    "    sr_total_error_1=np.sqrt(total_error_1)\n",
    "    sr_total_error_2=np.sqrt(total_error_2)\n",
    "    fin_error_1=(sr_total_error_1/(float(len(list_of_files))))\n",
    "    fin_error_2=(sr_total_error_2/(float(len(list_of_files))))\n",
    "    image_median_1 = np.median(data_array_1, axis=0)\n",
    "    image_median_2 = np.median(data_array_2, axis=0)\n",
    "    new_hdul = fits.HDUList()\n",
    "    new_hdul.append(fits.ImageHDU(image_median_1))\n",
    "    new_hdul.append(fits.ImageHDU(image_median_2))\n",
    "    new_hdul.writeto(out_file, overwrite=True)\n",
    "    #error\n",
    "    new_hdul = fits.HDUList()\n",
    "    new_hdul.append(fits.ImageHDU(fin_error_1))\n",
    "    new_hdul.append(fits.ImageHDU(fin_error_2))\n",
    "    new_hdul.writeto(error_file,overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-incident",
   "metadata": {},
   "source": [
    "To run the stacking code you need to run: stack(path_list, path_outfile, path_error_outfile)\n",
    "\n",
    "path_list - this is the list you create from the path column of the subset of the pandas database. \n",
    "\n",
    "path_outfile - this is the path and filename you want for your outfile. If you just put \"outfile.fits\" the file will save where you are running the notebook. I suggest start with the path to wfc3u so we keep them in a central location and to have the file be descriptive. Ex: \"/grp/hst/wfc3u/postflash_2021/2020_stack_flt.fits\"\n",
    "\n",
    "path_error_outfile - this is the path and filename you want for your outfile of the calcuated error. If you just put \"outfile.fits\" the file will save where you are running the notebook. I suggest start with the path to wfc3u so we keep them in a central location and to have the file be descriptive. Ex: \"/grp/hst/wfc3u/postflash_2021/2020_stack_error_flt.fits\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-scientist",
   "metadata": {},
   "source": [
    "Below I show how to create the proper cut for a single year of data from the pandas database which has already been cut to Fullframe, Medium current, Shutter A. \n",
    "\n",
    "From there I update the path_list, outfile, and error_outfile to be specfic to the year being tested. \n",
    "\n",
    "Once that is run, I use those parameters to run the `stack` function. Outputs will be saved to '/grp/hst/wfc3u/postflash_2021/'. \n",
    "\n",
    "I then update a copy of a new set to 2013 and run `stack` on that set. \n",
    "\n",
    "The following years/groups of years can be done the same way. Following the creation of all the stacked files we will still need to calculate the mean/median of the stacked images to plot them over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullframe_pf_A_2012 = fullframe_pf_A[(fullframe_pf_A['datetime'] > '2012-01-01 00:00:00') & (fullframe_pf_A['datetime'] < '2013-01-01 00:00:00')]\n",
    "paths_2012 = fullframe_pf_A_2012.path.tolist()\n",
    "outfile_2012 = '/grp/hst/wfc3u/postflash_2021/2012_fullframe_A_flt_stack.fits'\n",
    "error_outfile_2012 = '/grp/hst/wfc3u/postflash_2021/2012_fullframe_A_flt_error_stack.fits'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack(paths_2012, outfile, error_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullframe_pf_A_2013 = fullframe_pf_A[(fullframe_pf_A['datetime'] > '2013-01-01 00:00:00') & (fullframe_pf_A['datetime'] < '2014-01-01 00:00:00')]\n",
    "paths_2013 = fullframe_pf_A_2013.path.tolist()\n",
    "outfile_2013 = '/grp/hst/wfc3u/postflash_2021/2013_fullframe_A_flt_stack.fits'\n",
    "error_outfile_2013 = '/grp/hst/wfc3u/postflash_2021/2013_fullframe_A_flt_error_stack.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack(paths_2013, outfile_2013, error_outfile_2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-needle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
