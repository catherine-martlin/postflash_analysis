{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catherine Code - replaces all of sort_files.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postflash_data = pd.read_pickle('../../all_postflash_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullframe_pf = postflash_data.loc[(postflash_data['subarray'] == False)] \n",
    "fullframe_pf_A = postflash_data.loc[(postflash_data['subarray'] == False) & (postflash_data['shutter'] == 'A') & (postflash_data['flash_cur'] == 'MED')] \n",
    "fullframe_pf_B = postflash_data.loc[(postflash_data['subarray'] == False) & (postflash_data['shutter'] == 'B') & (postflash_data['flash_cur'] == 'MED')] \n",
    "A_paths = fullframe_pf_A.path.tolist()\n",
    "B_paths = fullframe_pf_B.path.tolist()\n",
    "\n",
    "#new_A_paths = []\n",
    "#for f in A_paths:\n",
    "#    new = '../../' + f\n",
    "#    new_A_paths.append(new)\n",
    "    \n",
    "#new_B_paths = []\n",
    "#for f in B_paths:\n",
    "#    new = '../../' + f\n",
    "#    new_B_paths.append(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heather's code - starting with stack.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the file size\n",
    "hdr = fits.getheader(new_A_paths[0], 1)\n",
    "nx = hdr['NAXIS1']\n",
    "ny = hdr['NAXIS2']\n",
    "nf = len(new_A_paths)\n",
    "set_data=fits.getdata(new_A_paths[0], 1)\n",
    "#makes empty array to be filled with data\n",
    "data_array_1 = np.empty((nf, ny, nx), dtype=float)\n",
    "data_array_2 = np.empty((nf, ny, nx), dtype=float)\n",
    "error_array_1 = np.empty((nf, ny, nx), dtype=float)\n",
    "error_array_2 = np.empty((nf, ny, nx), dtype=float)\n",
    "\n",
    "#makes array for rms\n",
    "rms_1 = np.zeros(len(new_A_paths), dtype=float)\n",
    "rms_2 = np.zeros(len(new_A_paths), dtype=float)\n",
    "total_error_1 = np.zeros_like(set_data, dtype=float)\n",
    "total_error_2 = np.zeros_like(set_data, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data and the DQs from the .fits for both extensions\n",
    "for i , f in enumerate(new_A_paths):\n",
    "    data_1=fits.getdata(f, 1)\n",
    "    data_2=fits.getdata(f, 4)\n",
    "    error_1=fits.getdata(f, 2)\n",
    "    error_2=fits.getdata(f, 5)\n",
    "    DQ_1=fits.getdata(f, 3)\n",
    "    DQ_2=fits.getdata(f, 6)\n",
    "\n",
    "#set the mask to a boolean array of the same size and shape as the data array    \n",
    "    mask_1=np.zeros_like(data_1, dtype=bool)\n",
    "    mask_2=np.zeros_like(data_2, dtype=bool)\n",
    "\n",
    "#I set the DQ to true because in the masking step it is understood that 1 will \n",
    "#be masked and zeros are fine so the logic in this step must be reversed.\n",
    "#create the mask for the data by setting any pixel with the flag value to true.\n",
    "    mask_1[(0<DQ_1)]= True\n",
    "    mask_2[(0<DQ_2)]= True\n",
    "    error_1[(0<DQ_1 & (DQ_1<2**13))]=0.00001\n",
    "    error_2[(0<DQ_2& (DQ_2<2**13))]=0.00001\n",
    "    error_1_sq=error_1**2\n",
    "    error_2_sq=error_2**2\n",
    "    \n",
    "#mask the data\n",
    "    masked_data_1= ma.array(data=data_1, mask=mask_1)\n",
    "    masked_data_2= ma.array(data=data_2, mask=mask_2)\n",
    "    \n",
    "#resets the data to array for stacking\n",
    "    data_array_1[i, :, :] = masked_data_1\n",
    "    rms_1[i] = masked_data_1.std()\n",
    "    data_array_2[i, :, :] = masked_data_2\n",
    "    rms_2[i] = masked_data_2.std()\n",
    "\n",
    "    total_error_1=total_error_1+(error_1_sq)\n",
    "    total_error_2=total_error_2+(error_2_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_error_1)\n",
    "sr_total_error_1=np.sqrt(total_error_1)\n",
    "sr_total_error_2=np.sqrt(total_error_2)\n",
    "fin_error_1=(sr_total_error_1/(float(len(new_A_paths))))\n",
    "fin_error_2=(sr_total_error_2/(float(len(new_A_paths))))\n",
    "\n",
    "#create the median image\n",
    "image_median_1 = np.median(data_array_1, axis=0)\n",
    "image_median_2 = np.median(data_array_2, axis=0)\n",
    "\n",
    "\n",
    "#create mean image\n",
    "image_mean_1= np.mean(data_array_1, axis=0)\n",
    "image_mean_2= np.mean(data_array_2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create wighted mean image\n",
    "#get wights\n",
    "weights_1 = 1./rms_1**2\n",
    "weights_1 /= weights_1.sum()\n",
    "\n",
    "weights_2 = 1./rms_2**2\n",
    "weights_2 /= weights_2.sum()\n",
    "\n",
    "\n",
    "weights_expand_1 = np.tile(weights_1[..., np.newaxis, np.newaxis], (1, ny, nx))\n",
    "weighted_mean_1 = np.sum(weights_expand_1 * data_array_1, axis=0)\n",
    "\n",
    "weights_expand_2 = np.tile(weights_2[..., np.newaxis, np.newaxis], (1, ny, nx))\n",
    "weighted_mean_2 = np.sum(weights_expand_2 * data_array_2, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "#prints stats\n",
    "print('image one', image_median_1.std(), image_mean_1.std(), weighted_mean_1.std())\n",
    "\n",
    "print('image two', image_median_2.std(), image_mean_2.std(), weighted_mean_2.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writes to the new file   \n",
    "\n",
    "#median\n",
    "new_hdul = fits.HDUList()\n",
    "new_hdul.append(fits.ImageHDU(image_median_1))\n",
    "new_hdul.append(fits.ImageHDU(image_median_2))\n",
    "new_hdul.writeto('A_median_pf.fits',overwrite=True)\n",
    "\n",
    "#error\n",
    "new_hdul = fits.HDUList()\n",
    "new_hdul.append(fits.ImageHDU(fin_error_1))\n",
    "new_hdul.append(fits.ImageHDU(fin_error_2))\n",
    "new_hdul.writeto('A_error_pf.fits',overwrite=True)\n",
    "    \n",
    "#mean\n",
    "new_hdul = fits.HDUList()\n",
    "new_hdul.append(fits.ImageHDU(image_mean_1))\n",
    "new_hdul.append(fits.ImageHDU(image_mean_2))\n",
    "new_hdul.writeto('A_mean_pf.fits',overwrite=True)\n",
    "\n",
    "\n",
    "#weighted mean\n",
    "new_hdul = fits.HDUList()\n",
    "new_hdul.append(fits.ImageHDU(weighted_mean_1))\n",
    "new_hdul.append(fits.ImageHDU(weighted_mean_2))\n",
    "new_hdul.writeto('A_weighted_mean_pf.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heather's code stack_yearly_v2.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack(list_of_files,out_file,error_file):\n",
    "    hdr = fits.getheader(list_of_files[0], 1)\n",
    "    nx = hdr['NAXIS1']\n",
    "    ny = hdr['NAXIS2']\n",
    "    nf = len(list_of_files)\n",
    "    data_array_1 = np.empty((nf, ny, nx), dtype=float)\n",
    "    data_array_2 = np.empty((nf, ny, nx), dtype=float)\n",
    "    set_data=fits.getdata(list_of_files[0], 1)\n",
    "    rms_1 = np.zeros(len(list_of_files), dtype=float)\n",
    "    rms_2 = np.zeros(len(list_of_files), dtype=float)\n",
    "    error_array_1 = np.empty((nf, ny, nx), dtype=float)\n",
    "    error_array_2 = np.empty((nf, ny, nx), dtype=float)\n",
    "    total_error_1 = np.zeros_like(set_data, dtype=float)\n",
    "    total_error_2 = np.zeros_like(set_data, dtype=float)\n",
    "    for i , f in enumerate(list_of_files):\n",
    "        data_1=fits.getdata(f, 1)\n",
    "        data_2=fits.getdata(f, 4)\n",
    "        error_1=fits.getdata(f, 2)\n",
    "        error_2=fits.getdata(f, 5)\n",
    "        DQ_1=fits.getdata(f, 3)\n",
    "        DQ_2=fits.getdata(f, 6)\n",
    "        #data_1=data1\n",
    "        #data_2=data2\n",
    "        mask_1=np.zeros_like(data_1, dtype=bool)\n",
    "        mask_2=np.zeros_like(data_2, dtype=bool)\n",
    "        mask_1[DQ_1>=2**13]= True\n",
    "        mask_2[DQ_2>=2**13]= True\n",
    "        error_1[(0<DQ_1 & (DQ_1<2**13))]=0.00001\n",
    "        error_2[(0<DQ_2& (DQ_2<2**13))]=0.00001\n",
    "        error_1_sq=error_1**2\n",
    "        error_2_sq=error_2**2\n",
    "        masked_data_1= ma.array(data=data_1, mask=mask_1)\n",
    "        masked_data_2= ma.array(data=data_2, mask=mask_2)\n",
    "        data_array_1[i, :, :] = masked_data_1\n",
    "        rms_1[i] = masked_data_1.std()\n",
    "        data_array_2[i, :, :] = masked_data_2\n",
    "        rms_2[i] = masked_data_2.std()\n",
    "        total_error_1=total_error_1+(error_1_sq)\n",
    "        total_error_2=total_error_2+(error_2_sq)\n",
    "    sr_total_error_1=np.sqrt(total_error_1)\n",
    "    sr_total_error_2=np.sqrt(total_error_2)\n",
    "    fin_error_1=(sr_total_error_1/(float(len(list_of_files))))\n",
    "    fin_error_2=(sr_total_error_2/(float(len(list_of_files))))\n",
    "    image_median_1 = np.median(data_array_1, axis=0)\n",
    "    image_median_2 = np.median(data_array_2, axis=0)\n",
    "    new_hdul = fits.HDUList()\n",
    "    new_hdul.append(fits.ImageHDU(image_median_1))\n",
    "    new_hdul.append(fits.ImageHDU(image_median_2))\n",
    "    new_hdul.writeto(out_file, overwrite=True)\n",
    "    #error\n",
    "    new_hdul = fits.HDUList()\n",
    "    new_hdul.append(fits.ImageHDU(fin_error_1))\n",
    "    new_hdul.append(fits.ImageHDU(fin_error_2))\n",
    "    new_hdul.writeto(error_file,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    stack(new_A_paths,'A_median_test_on_stack_yearly.fits','A_error_test_on_stack_yearly.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
